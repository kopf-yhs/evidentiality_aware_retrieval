{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import statistics\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE=''\n",
    "GOLD_PASSAGE_FILE=''\n",
    "RESULT_FILE=''\n",
    "OUTPUT_CF_TRAIN_PATH=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Span Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "random.seed(0)\n",
    "\n",
    "make_random_mask = False\n",
    "exact_answers = []\n",
    "not_include_answer = []\n",
    "span_len = 16\n",
    "WINDOWING=False\n",
    "\n",
    "def find_and_mask(answers, txt):\n",
    "    masked_txt = txt\n",
    "    masked=False\n",
    "    for ans in answers:\n",
    "        ans = re.sub(r'([^a-zA-Z0-9,\\.!\\? -])',r'\\\\\\1',ans)\n",
    "        ans_len_char = len(ans)\n",
    "\n",
    "        ans_split = ans.split(' ')\n",
    "        ans_len_token = len(ans_split)\n",
    "\n",
    "        re_ans = '( )?'.join(ans_split)\n",
    "        re_ans = re.compile(re_ans,re.I)\n",
    "        start_idxs = list(re.finditer(re_ans, masked_txt))\n",
    "        ans_spans = [(-1,0)] + [(start_idx.start(), start_idx.start() + ans_len_char) for start_idx in start_idxs] + [(len(masked_txt),-1)]\n",
    "\n",
    "        if len(start_idxs) > 0:\n",
    "            new_masked_txt = ''\n",
    "            for i, _ in enumerate(ans_spans[:-1]):\n",
    "                s_idx = ans_spans[i][1]\n",
    "                e_idx = ans_spans[i+1][0]\n",
    "                new_masked_txt += masked_txt[s_idx:e_idx]+' '\n",
    "            masked_txt = new_masked_txt\n",
    "            masked=True\n",
    "\n",
    "    if masked:\n",
    "        return masked_txt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizing Counterfactual Passages for AAR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(GOLD_PASSAGE_FILE,'r') as f:\n",
    "    raw_gold_passages = json.load(f)\n",
    "raw_gold_passages = raw_gold_passages['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding any entries in gold_passage_info without corresponding context information\n",
    "gold_passages = list()\n",
    "for gold in raw_gold_passages:\n",
    "    if gold['context']:\n",
    "        gold_passages.append(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import copy\n",
    "import nltk\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "removed_percentage = []\n",
    "masked_gold_infos = []\n",
    "count = 0\n",
    "\n",
    "for iter,row in tqdm(enumerate(gold_passages)):\n",
    "    original_passage = row['context']\n",
    "    passage=''\n",
    "    masked_passage = ''\n",
    "    short_answer =  ''\n",
    "    answer_sentence = ''\n",
    "    answer_masked_passage = ''\n",
    "    answer_percent=-1.0\n",
    "    masked = False\n",
    "\n",
    "    answer_mask = find_and_mask(row['short_answers'], original_passage)\n",
    "    if answer_mask:\n",
    "        answer_masked_passage = answer_mask\n",
    "    \n",
    "    passage = original_passage\n",
    "    original_sentences = nltk.tokenize.sent_tokenize(original_passage)\n",
    "    for sent_idx, sentence in enumerate(original_sentences):\n",
    "        ans=row['short_answers']\n",
    "        if find_and_mask(ans, sentence):\n",
    "            cf_sentences = copy.deepcopy(original_sentences)\n",
    "            cf_sentences.remove(sentence)\n",
    "            \n",
    "            if len(cf_sentences) <= 0:\n",
    "                count += 1\n",
    "                break\n",
    "            masked_passage = ' '.join(cf_sentences)\n",
    "            \n",
    "            len_passage = len(nltk.word_tokenize(passage))\n",
    "            len_masked_passage = len(nltk.word_tokenize(masked_passage))\n",
    "            tok_diff = len_passage - len_masked_passage\n",
    "            \n",
    "            rest_sents = ' '.join(nltk.word_tokenize(masked_passage)[:-tok_diff])\n",
    "            rest_sents = nltk.sent_tokenize(rest_sents)\n",
    "            original_sents = rest_sents[:sent_idx] + [sentence] +rest_sents[sent_idx:]\n",
    "            trunc_passage = ' '.join(original_sents)\n",
    "            \n",
    "            short_answer = copy.deepcopy(ans)\n",
    "            answer_sentence = copy.deepcopy(sentence)\n",
    "            answer_percent = (len(nltk.word_tokenize(answer_sentence))/len(nltk.word_tokenize(passage))*100)\n",
    "            masked=True\n",
    "            break\n",
    "\n",
    "    if masked:\n",
    "        masked_row = copy.deepcopy(row)\n",
    "        masked_row['qid'] = iter\n",
    "        masked_row['short_answers'] = short_answer\n",
    "        masked_row['answer_sentence'] = answer_sentence\n",
    "        masked_row['answer_mask_passage'] = answer_masked_passage\n",
    "        masked_row['sentence_mask_passage'] = masked_passage\n",
    "        masked_row['answer_passage_trunc'] = trunc_passage\n",
    "        masked_row['answer_percent'] = answer_percent\n",
    "        masked_row['token_difference'] = tok_diff\n",
    "        masked_gold_infos.append(masked_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('que_psg_ans_counterfactual_gold_info.csv','w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for line in masked_gold_infos:\n",
    "        writer.writerow([\n",
    "                line['qid'], \n",
    "                line['question'], \n",
    "                '', \n",
    "                line['context'], \n",
    "                line['title'], \n",
    "                line['answer_mask_passage'], \n",
    "                line['sentence_mask_passage'], \n",
    "                line['answer_passage_trunc'], \n",
    "                line['short_answers'], \n",
    "                line['answer_sentence'], \n",
    "                line['answer_percent'],\n",
    "                line['token_difference']\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizing Counterfactual Passages for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(TRAIN_FILE,'r') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import copy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "cf_data = list()\n",
    "\n",
    "for iter,row in tqdm(enumerate(train_data)):\n",
    "    answers = row['answers']\n",
    "    entry = copy.deepcopy(row)\n",
    "    entry['positive_ctxs'] = list()\n",
    "    entry['cf_negative_ctxs'] = list()\n",
    "\n",
    "    for j, txt in enumerate(row['positive_ctxs']):\n",
    "        original_passage = txt['text']\n",
    "        original_title = txt['title']\n",
    "        original_sentences = nltk.tokenize.sent_tokenize(original_passage)\n",
    "        \n",
    "        masked_passage = ''\n",
    "        nonanswer_sentences = list()\n",
    "        nonanswer_title=original_title+''\n",
    "        \n",
    "        for sentence in original_sentences:\n",
    "            if not find_and_mask(answers, sentence):\n",
    "                nonanswer_sentences.append(sentence)\n",
    "\n",
    "        if original_title:\n",
    "            for ans in answers:\n",
    "                try :\n",
    "                    nonanswer_title=re.sub(ans,'',nonanswer_title)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        if nonanswer_sentences:\n",
    "            entry['cf_negative_ctxs'].append({\n",
    "                'title':nonanswer_title,\n",
    "                'text':' '.join(nonanswer_sentences)\n",
    "            })\n",
    "            entry['positive_ctxs'].append(txt)\n",
    "    cf_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_CF_TRAIN_PATH,'w') as f:\n",
    "    json.dump(cf_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yhs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "521ada6ce993e60f4929eef7bf9c2973b2895573f42c0bf4970658dab7fb8207"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
